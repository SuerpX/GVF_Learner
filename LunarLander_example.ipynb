{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_mod_cartpole\n",
    "from random import random, randint, uniform\n",
    "from env.decom_lunar_lander import LunarLander as LunarLander_decom_reward\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GVF_learner import GVF_learner\n",
    "from memory.memory import ReplayBuffer_decom\n",
    "from models.dqn_model import DQNModel\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "# Writer = SummaryWriter(log_dir=\"CartPole_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV_NAME = 'CartPoleMod-v0'\n",
    "ENV_NAME = 'LunarLander_decom'\n",
    "env = LunarLander_decom_reward()\n",
    "# env._max_episode_steps = 500\n",
    "ACTION_DICT = {\n",
    "    \"NOOP\": 0,\n",
    "    \"LEFT\":1,\n",
    "    \"MAIN\":2,\n",
    "    \"RIGHT\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set result saveing floder\n",
    "result_floder = ENV_NAME\n",
    "result_file = ENV_NAME + \"/results.txt\"\n",
    "if not os.path.isdir(result_floder):\n",
    "    os.mkdir(result_floder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_Lunarlander = {\n",
    "    'epsilon_decay_steps' : 200000, \n",
    "    'final_epsilon' : 0.01,\n",
    "    'batch_size' : 128, \n",
    "    'update_steps' : 3, \n",
    "    'memory_size' : 100000, \n",
    "    'beta' : 0.99, \n",
    "    'model_replace_freq' : 1,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'decom_reward_len': 8,\n",
    "    'soft_tau': 5e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent(object):\n",
    "    def __init__(self, env, hyper_params, action_space = len(ACTION_DICT)):\n",
    "        \n",
    "        self.env = env\n",
    "        self.max_episode_steps = env._max_episode_steps\n",
    "        \n",
    "        \"\"\"\n",
    "            beta: The discounted factor of Q-value function\n",
    "            (epsilon): The explore or exploit policy epsilon. \n",
    "            initial_epsilon: When the 'steps' is 0, the epsilon is initial_epsilon, 1\n",
    "            final_epsilon: After the number of 'steps' reach 'epsilon_decay_steps', \n",
    "                The epsilon set to the 'final_epsilon' determinately.\n",
    "            epsilon_decay_steps: The epsilon will decrease linearly along with the steps from 0 to 'epsilon_decay_steps'.\n",
    "        \"\"\"\n",
    "        self.beta = hyper_params['beta']\n",
    "        self.initial_epsilon = 1\n",
    "        self.final_epsilon = hyper_params['final_epsilon']\n",
    "        self.epsilon_decay_steps = hyper_params['epsilon_decay_steps']\n",
    "        self.soft_tau = hyper_params['soft_tau']\n",
    "\n",
    "        \"\"\"\n",
    "            episode: Record training episode\n",
    "            steps: Add 1 when predicting an action\n",
    "            learning: The trigger of agent learning. It is on while training agent. It is off while testing agent.\n",
    "            action_space: The action space of the current environment, e.g 2.\n",
    "        \"\"\"\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.best_reward = -float(\"inf\")\n",
    "        self.action_space = action_space\n",
    "\n",
    "        \"\"\"\n",
    "            input_len The input length of the neural network. It equals to the length of the state vector.\n",
    "            output_len: The output length of the neural network. It is equal to the action space.\n",
    "            eval_model: The model for predicting action for the agent.\n",
    "            target_model: The model for calculating Q-value of next_state to update 'eval_model'.\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        self.state_len = len(state)\n",
    "        input_len = self.state_len + action_space\n",
    "        output_len = 1\n",
    "        self.decom_reward_len = hyper_params[\"decom_reward_len\"]\n",
    "        \n",
    "        self.action_vector = self.get_action_vector()\n",
    "        self.eval_model = DQNModel(input_len, output_len, learning_rate = hyper_params['learning_rate'])\n",
    "        self.target_model = DQNModel(input_len, output_len)\n",
    "        \n",
    "#         memory: Store and sample experience replay.\n",
    "        self.memory = ReplayBuffer_decom(hyper_params['memory_size'])\n",
    "        \n",
    "        \"\"\"\n",
    "            batch_size: Mini batch size for training model.\n",
    "            update_steps: The frequence of traning model\n",
    "            model_replace_freq: The frequence of replacing 'target_model' by 'eval_model'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.update_steps = hyper_params['update_steps']\n",
    "        self.model_replace_freq = hyper_params['model_replace_freq']\n",
    "        \n",
    "#         if os.path.isdir(\"CartPole_summary/Lunarlander/DQN(unconstraint)/\"):\n",
    "#             shutil.rmtree(\"CartPole_summary/Lunarlander/DQN(unconstraint)/\")\n",
    "        \n",
    "        \n",
    "    # Linear decrease function for epsilon\n",
    "    def linear_decrease(self, initial_value, final_value, curr_steps, final_decay_steps):\n",
    "        decay_rate = curr_steps / final_decay_steps\n",
    "        if decay_rate > 1:\n",
    "            decay_rate = 1\n",
    "        return initial_value - (initial_value - final_value) * decay_rate\n",
    "    \n",
    "    def get_action_vector(self):\n",
    "        action_vector = np.zeros((self.action_space, self.action_space))\n",
    "        for i in range(len(action_vector)):\n",
    "            action_vector[i, i] = 1\n",
    "        \n",
    "        return FloatTensor(action_vector)\n",
    "    \n",
    "    def concat_state_action(self, states, actions = None, is_full_action = False):\n",
    "        if is_full_action:\n",
    "            com_state = FloatTensor(states).repeat((1, self.action_space)).view((-1, self.state_len))\n",
    "            actions = self.action_vector.repeat((len(states), 1))\n",
    "        else:\n",
    "            com_state = states.clone()\n",
    "            actions = actions.clone()\n",
    "        state_action = torch.cat((com_state, actions), 1)\n",
    "        return state_action\n",
    "        \n",
    "    def explore_or_exploit_policy(self, state):\n",
    "        p = uniform(0, 1)\n",
    "        # Get decreased epsilon\n",
    "        epsilon = self.linear_decrease(self.initial_epsilon, \n",
    "                               self.final_epsilon,\n",
    "                               self.steps,\n",
    "                               self.epsilon_decay_steps)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        if p < epsilon:\n",
    "            return randint(0, self.action_space - 1)\n",
    "        else:\n",
    "            return self.greedy_policy(state)[0]\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        state_ft = FloatTensor(state).view(-1, self.state_len)\n",
    "        state_action = self.concat_state_action(state_ft, is_full_action = True)\n",
    "        feature_vectors, q_values = self.eval_model.predict_batch(state_action)\n",
    "        q_v, best_action = q_values.max(0)\n",
    "        return best_action.item(), q_v, feature_vectors[best_action.item()]\n",
    "    \n",
    "    def update_batch(self):\n",
    "#         print(self.update_steps)\n",
    "        if len(self.memory) < self.batch_size or self.steps % self.update_steps != 0:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        (states_actions, _, reward, next_states,\n",
    "         is_terminal, _) = batch\n",
    "        \n",
    "#         states_actions = states_actions\n",
    "        next_states = FloatTensor(next_states)\n",
    "        terminal = FloatTensor([1 if t else 0 for t in is_terminal])\n",
    "        reward = FloatTensor(reward)\n",
    "        \n",
    "        batch_index = torch.arange(self.batch_size,\n",
    "                                   dtype=torch.long)\n",
    "        \n",
    "        # Current Q Values\n",
    "        _, q_values = self.eval_model.predict_batch(states_actions)\n",
    "        next_state_actions = self.concat_state_action(next_states, is_full_action = True)\n",
    "        _, q_next = self.target_model.predict_batch(next_state_actions)\n",
    "        q_next = q_next.view((-1, self.action_space))\n",
    "        q_max, idx = q_next.detach().max(1)\n",
    "\n",
    "        q_max = (1 - terminal) * q_max\n",
    "        q_target = reward + self.beta * q_max\n",
    "        q_target = q_target.unsqueeze(1)\n",
    "        \n",
    "        self.eval_model.fit(q_values, q_target)\n",
    "        \n",
    "    def learn_and_evaluate(self, training_episodes, test_interval):\n",
    "        test_number = training_episodes // test_interval\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(test_number):\n",
    "            # learn\n",
    "            self.learn(test_interval)\n",
    "            # evaluate\n",
    "            avg_reward = self.evaluate((i + 1) * test_interval)\n",
    "            all_results.append(avg_reward)\n",
    "            \n",
    "        return all_results\n",
    "    \n",
    "    def get_features_decom(self, state, next_state, done):\n",
    "        \n",
    "        threshold_x = 1\n",
    "        threshold_c_v = 1\n",
    "        threshold_angle = 0.07\n",
    "        threshold_p_v = 0.7\n",
    "        \n",
    "        features_decom = np.ones(self.decom_reward_len)\n",
    "#         cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
    "        next_cart_position, next_cart_velocity, next_pole_angle, next_pole_velocity = next_state\n",
    "            \n",
    "        if threshold_x < next_cart_position:\n",
    "            features_decom[0] = -1\n",
    "        if -threshold_x > next_cart_position:\n",
    "            features_decom[1] = -1        \n",
    "    \n",
    "        if threshold_c_v < next_cart_velocity:\n",
    "            features_decom[2] = -1\n",
    "        if -threshold_c_v > next_cart_velocity:\n",
    "            features_decom[3] = -1   \n",
    "            \n",
    "        if threshold_angle < next_pole_angle:\n",
    "            features_decom[4] = -1\n",
    "        if -threshold_angle > next_pole_angle:\n",
    "            features_decom[5] = -1   \n",
    "        \n",
    "        if threshold_p_v < next_pole_velocity:\n",
    "            features_decom[6] = -1\n",
    "        if -threshold_p_v > next_pole_velocity:\n",
    "            features_decom[7] = -1   \n",
    "        return features_decom\n",
    "    \n",
    "    def learn(self, test_interval):\n",
    "        \n",
    "        for episode in tqdm(range(test_interval), desc=\"Training\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                steps += 1\n",
    "                self.steps += 1\n",
    "                \n",
    "                action = self.explore_or_exploit_policy(state)\n",
    "                next_state, reward, done, _, features_decom = self.env.step(action)\n",
    "                \n",
    "#                 features_decom = self.get_features_decom(state, next_state, steps < self.max_episode_steps and done)\n",
    "                action_vector = np.zeros(self.action_space)\n",
    "                action_vector[action] = 1\n",
    "                \n",
    "                self.memory.add(np.concatenate((state.copy(), action_vector.copy()), axis=0), -1, reward, next_state.copy(), steps < self.max_episode_steps and done, features_decom)\n",
    "                self.update_batch()\n",
    "                \n",
    "                if self.steps % self.model_replace_freq == 0:\n",
    "                    if self.model_replace_freq == 1:\n",
    "                        self.target_model.replace_soft(self.eval_model, tau = self.soft_tau)\n",
    "                    else:\n",
    "                        self.target_model.replace(self.eval_model)\n",
    "                state = next_state\n",
    "\n",
    "    def evaluate(self, episode_num, trials = 10):\n",
    "        total_reward = 0\n",
    "        for _ in tqdm(range(trials), desc=\"Evaluating\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                steps += 1\n",
    "                action = self.greedy_policy(state)[0]\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "            \n",
    "        avg_reward = total_reward / trials\n",
    "        print(avg_reward)\n",
    "        print(\"storage len:\", len(self.memory))\n",
    "        if avg_reward >= self.best_reward:\n",
    "            self.best_reward = avg_reward\n",
    "            self.save_model()\n",
    "            print(\"save\")\n",
    "#         Writer.add_scalars(main_tag='CartPole/DQN',\n",
    "#                                 tag_scalar_dict = {'DQN(unconstraint)':avg_reward}, \n",
    "# #                                 scalar_value=,\n",
    "#                                 global_step=episode_num)\n",
    "    \n",
    "        return avg_reward\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.eval_model.save(result_floder + '/best_model.pt')\n",
    "        self.memory.save(result_floder)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.eval_model.load(result_floder + '/best_model.pt')\n",
    "        self.memory.load(result_floder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Cart Pole DQN agent\n",
    "Generating policy and feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 2/100 [00:00<00:14,  6.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fdeafdfc1f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams_Lunarlander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e17105febf18>\u001b[0m in \u001b[0;36mlearn_and_evaluate\u001b[0;34m(self, training_episodes, test_interval)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e17105febf18>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, test_interval)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore_or_exploit_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_decom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m#                 features_decom = self.get_features_decom(state, next_state, steps < self.max_episode_steps and done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/GVF_learner/env/decom_lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/GVF_learner/env/decom_lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_episodes, test_interval = 10000, 100\n",
    "agent = DQN_agent(env, hyperparams_Lunarlander)\n",
    "result = agent.learn_and_evaluate(training_episodes, test_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GVF learner\n",
    "Train GVF model base on the dataset and policy above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "CP_GVF_PARAMETERS = {\n",
    "    \"batch size\" : 128, # update batch size\n",
    "    \"learning rate\" : 0.0001,\n",
    "    \"feature num\" : 8, # numbers/length of feature\n",
    "    \"state length\" : 8,\n",
    "    \"discount factor\" : [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99], # for each features respectively\n",
    "    \"action space\": 4,\n",
    "#     'model_replace_freq' : 300,\n",
    "#     'soft_tau': 0.001\n",
    "    'model_replace_freq' : 100,\n",
    "    'soft_tau': 5e-4\n",
    "}\n",
    "dqn = DQN_agent(env, hyperparams_Lunarlander)\n",
    "dqn.load_model()\n",
    "def policy(state_actions):\n",
    "#     print(1)\n",
    "#     print(state_actions.size())\n",
    "    _, q_next = dqn.eval_model.predict_batch(state_actions)\n",
    "#     print(q_next.size())\n",
    "    q_next = q_next.view((-1, dqn.action_space))\n",
    "#     print(q_next)\n",
    "    q_max, idx = q_next.detach().max(1)\n",
    "#     print(3)\n",
    "#     print(q_max)\n",
    "#     print(idx)\n",
    "#     input()\n",
    "    return idx\n",
    "\n",
    "def get_dataset():\n",
    "    dataset = dqn.memory._storage\n",
    "    print(dataset[:10])\n",
    "    return np.array(dataset).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.10382108,  0.60593271,  0.14626195, -0.67588508,  0.06564762,\n",
      "       -0.0192917 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 1.4214518519328976, array([ 0.10537729,  0.5908259 ,  0.15596962, -0.67139524,  0.06530218,\n",
      "       -0.00690881,  0.        ,  0.        ], dtype=float32), False, [-0.014613074054139696, -0.00225600574639373, -0.00034543871879577637, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 0.10537729,  0.59082592,  0.15596962, -0.67139524,  0.06530218,\n",
      "       -0.00690881,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 4.328498174842946, array([ 0.10685253,  0.5764145 ,  0.14798655, -0.6404877 ,  0.06484399,\n",
      "       -0.00916355,  0.        ,  0.        ], dtype=float32), False, [-0.01391495120266295, -0.031911842190797124, -0.00045818835496902466, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 0.10685253,  0.57641453,  0.14798655, -0.64048767,  0.06484399,\n",
      "       -0.00916355,  0.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  0.        ]), -1, -1.1844358396409678, array([ 0.1082593 ,  0.5613917 ,  0.13939163, -0.6677382 ,  0.06611778,\n",
      "        0.02547584,  0.        ,  0.        ], dtype=float32), False, [-0.014499885594411621, 0.024770455479068088, 0.0012737885117530823, 0.0, 0.0, 0.0, 0.01, 0]), (array([ 0.1082593 ,  0.56139171,  0.13939163, -0.6677382 ,  0.06611778,\n",
      "        0.02547584,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  1.        ]), -1, -1.2556587461077686, array([ 0.10973539,  0.5457699 ,  0.14807804, -0.6942805 ,  0.06565271,\n",
      "       -0.00930142,  0.        ,  0.        ], dtype=float32), False, [-0.015042268199368602, 0.027763920901306305, -0.00046506524085998535, 0.0, 0.0, 0.0, 0.01, 0]), (array([ 0.10973539,  0.54576987,  0.14807804, -0.69428051,  0.06565271,\n",
      "       -0.00930142,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ]), -1, -1.0043471818431726, array([ 0.11121149,  0.52954817,  0.14807804, -0.72094715,  0.06518764,\n",
      "       -0.00930139,  0.        ,  0.        ], dtype=float32), False, [-0.015592528663888272, 0.026101073173760425, -0.0004650726914405823, 0.0, 0.0, 0.0, 0.0, 0]), (array([ 0.11121149,  0.52954817,  0.14807804, -0.72094715,  0.06518764,\n",
      "       -0.00930139,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 3.982717669916423, array([ 1.12738416e-01,  5.13992131e-01,  1.52676880e-01, -6.91377342e-01,\n",
      "        6.52123615e-02,  4.94561507e-04,  0.00000000e+00,  0.00000000e+00],\n",
      "      dtype=float32), False, [-0.014889180092175103, -0.02796271763340963, 2.4721026420593262e-05, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 1.12738416e-01,  5.13992131e-01,  1.52676880e-01, -6.91377342e-01,\n",
      "        6.52123615e-02,  4.94561507e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00]), -1, 2.6151866639744243, array([ 0.1143261 ,  0.49881577,  0.15824947, -0.67452794,  0.0657137 ,\n",
      "        0.01002662,  0.        ,  0.        ], dtype=float32), False, [-0.014461283578702022, -0.015191917728248239, 0.0005013346672058105, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 0.1143261 ,  0.49881577,  0.15824947, -0.67452794,  0.0657137 ,\n",
      "        0.01002662,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 2.8920026334765625, array([ 0.11569758,  0.48393682,  0.13746968, -0.66127324,  0.06539977,\n",
      "       -0.00627858,  0.        ,  0.        ], dtype=float32), False, [-0.014174696673570075, -0.017431406898324453, -0.0003139227628707886, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 0.11569758,  0.48393682,  0.13746968, -0.66127324,  0.06539977,\n",
      "       -0.00627858,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 3.61442909098663, array([ 0.11717834,  0.46971083,  0.1476279 , -0.6322866 ,  0.06584296,\n",
      "        0.00886378,  0.        ,  0.        ], dtype=float32), False, [-0.013468487837928811, -0.026118993408164926, 0.000443190336227417, 0.0, 0.0, 0.01, 0.0, 0]), (array([ 0.11717834,  0.46971083,  0.14762791, -0.63228661,  0.06584296,\n",
      "        0.00886378,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        1.        ,  0.        ]), -1, 0.4161434428054463, array([ 0.11877155,  0.45542228,  0.1582375 , -0.6350946 ,  0.0669144 ,\n",
      "        0.02142874,  0.        ,  0.        ], dtype=float32), False, [-0.013451387922645164, 0.0052185153012692664, 0.001071438193321228, 0.0, 0.0, 0.01, 0.0, 0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 1/1000 [00:03<58:34,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.003196578075243023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:36<54:42,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004636180208837542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [01:09<55:03,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00037199559958253886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [01:43<54:15,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003664212837133317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [02:17<53:58,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003526521075381915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [02:50<53:13,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003497677548542706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [03:24<52:26,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00035120272828197396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [03:57<51:39,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003643697391662662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [04:31<50:59,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.000363384985345695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 91/1000 [05:04<50:33,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003498347004285425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [05:37<49:58,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00034310379487578757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111/1000 [06:11<49:43,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003572095902432697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [06:45<49:25,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00035589936851000394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131/1000 [07:18<48:52,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003597012577269374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 141/1000 [07:52<48:08,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003550283631401868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 151/1000 [08:26<48:01,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003536543958118338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 161/1000 [09:00<47:43,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00035802731049106257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 171/1000 [09:33<46:34,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00036100297322375595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 181/1000 [10:08<48:29,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00035819332916079663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 191/1000 [10:43<46:15,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00036647299716338134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [11:17<44:36,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003508519522021459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 211/1000 [11:50<44:09,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003873337318276977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 221/1000 [12:24<43:26,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00036987531978032275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 231/1000 [12:57<42:41,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003675684486963915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 241/1000 [13:31<43:06,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003709778020748972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 251/1000 [14:06<43:19,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00038518775140324306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 261/1000 [14:40<41:44,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003844504587954003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271/1000 [15:14<41:05,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00039342637697104937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 281/1000 [15:48<40:39,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00037602953957584315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 291/1000 [16:21<39:45,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00038580651468861745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [16:55<39:24,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004248920443055132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 311/1000 [17:29<38:27,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004056327092434343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 321/1000 [18:02<37:56,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00039802700009420596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 331/1000 [18:36<37:15,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004469767594194907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 341/1000 [19:09<36:53,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004284293128150281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 351/1000 [19:43<36:42,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004055026495099619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 361/1000 [20:17<36:44,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003913175769923928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 371/1000 [20:51<35:17,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.000407024529222396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 381/1000 [21:25<34:57,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0003798374208602179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 391/1000 [21:58<34:07,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004144185302888348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [22:32<33:19,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00041388103841258215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 411/1000 [23:05<33:17,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004252979950372712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421/1000 [23:39<32:18,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004048300931020878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 431/1000 [24:13<31:52,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00040890725002732916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441/1000 [24:47<31:21,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00048215793622078556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 451/1000 [25:21<31:30,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004322719939067648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 461/1000 [25:54<30:18,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004144295462282271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 471/1000 [26:28<29:59,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004117388008588888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 481/1000 [27:02<29:14,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004516238655933403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 491/1000 [27:36<28:33,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004906936722193949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [28:10<27:54,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004827405004400362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 511/1000 [28:43<27:36,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004720130416950511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 521/1000 [29:17<26:40,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00044805536241195574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 531/1000 [29:50<26:02,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004552314112562409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 541/1000 [30:23<25:24,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004845858985272175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 551/1000 [30:57<24:51,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0004974324048789013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [31:30<24:44,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.00048524845778793834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 571/1000 [32:04<23:44,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0005126559171680352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 581/1000 [32:37<23:19,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.0005297528853675093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-10f24c80ef7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgvf_learner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGVF_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCP_GVF_PARAMETERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgvf_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/research/GVF_learner/GVF_learner.py\u001b[0m in \u001b[0;36mlearn_and_eval\u001b[0;34m(self, train_epochs, test_interval)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/GVF_learner/GVF_learner.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/general/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/general/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/general/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/general/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = get_dataset()\n",
    "gvf_learner = GVF_learner(CP_GVF_PARAMETERS, dataset, policy)\n",
    "gvf_learner.learn_and_eval(1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "tensor([[ 0.0022,  0.0017, -0.0011, -0.0634, -0.1049,  0.0058, -0.0025,  0.7100]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = torch.tensor([[0.015588092617690563, -0.0004975938936695457, 4.084892424316422e-08, 6.387576578781307e-10, 0.0024009563494473696, -9.596109151743804e-08, 1, 1, 0, 0, 1, 0]]).cuda()\n",
    "print(test_data.size())\n",
    "r = gvf_learner.eval_model(test_data)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

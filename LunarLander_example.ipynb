{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_mod_cartpole\n",
    "from random import random, randint, uniform\n",
    "from env.decom_lunar_lander import LunarLander as LunarLander_decom_reward\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GVF_learner import GVF_learner\n",
    "from memory.memory import ReplayBuffer_decom\n",
    "from models.dqn_model import DQNModel\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "# Writer = SummaryWriter(log_dir=\"CartPole_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV_NAME = 'CartPoleMod-v0'\n",
    "ENV_NAME = 'LunarLander_decom'\n",
    "env = LunarLander_decom_reward()\n",
    "# env._max_episode_steps = 500\n",
    "ACTION_DICT = {\n",
    "    \"NOOP\": 0,\n",
    "    \"LEFT\":1,\n",
    "    \"MAIN\":2,\n",
    "    \"RIGHT\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set result saveing floder\n",
    "result_floder = ENV_NAME\n",
    "result_file = ENV_NAME + \"/results.txt\"\n",
    "if not os.path.isdir(result_floder):\n",
    "    os.mkdir(result_floder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_Lunarlander = {\n",
    "    'epsilon_decay_steps' : 200000, \n",
    "    'final_epsilon' : 0.01,\n",
    "    'batch_size' : 128, \n",
    "    'update_steps' : 3, \n",
    "    'memory_size' : 100000, \n",
    "    'beta' : 0.99, \n",
    "    'model_replace_freq' : 1,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'decom_reward_len': 8,\n",
    "    'soft_tau': 5e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent(object):\n",
    "    def __init__(self, env, hyper_params, action_space = len(ACTION_DICT)):\n",
    "        \n",
    "        self.env = env\n",
    "        self.max_episode_steps = env._max_episode_steps\n",
    "        \n",
    "        \"\"\"\n",
    "            beta: The discounted factor of Q-value function\n",
    "            (epsilon): The explore or exploit policy epsilon. \n",
    "            initial_epsilon: When the 'steps' is 0, the epsilon is initial_epsilon, 1\n",
    "            final_epsilon: After the number of 'steps' reach 'epsilon_decay_steps', \n",
    "                The epsilon set to the 'final_epsilon' determinately.\n",
    "            epsilon_decay_steps: The epsilon will decrease linearly along with the steps from 0 to 'epsilon_decay_steps'.\n",
    "        \"\"\"\n",
    "        self.beta = hyper_params['beta']\n",
    "        self.initial_epsilon = 1\n",
    "        self.final_epsilon = hyper_params['final_epsilon']\n",
    "        self.epsilon_decay_steps = hyper_params['epsilon_decay_steps']\n",
    "        self.soft_tau = hyper_params['soft_tau']\n",
    "\n",
    "        \"\"\"\n",
    "            episode: Record training episode\n",
    "            steps: Add 1 when predicting an action\n",
    "            learning: The trigger of agent learning. It is on while training agent. It is off while testing agent.\n",
    "            action_space: The action space of the current environment, e.g 2.\n",
    "        \"\"\"\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.best_reward = -float(\"inf\")\n",
    "        self.action_space = action_space\n",
    "\n",
    "        \"\"\"\n",
    "            input_len The input length of the neural network. It equals to the length of the state vector.\n",
    "            output_len: The output length of the neural network. It is equal to the action space.\n",
    "            eval_model: The model for predicting action for the agent.\n",
    "            target_model: The model for calculating Q-value of next_state to update 'eval_model'.\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        self.state_len = len(state)\n",
    "        input_len = self.state_len + action_space\n",
    "        output_len = 1\n",
    "        self.decom_reward_len = hyper_params[\"decom_reward_len\"]\n",
    "        \n",
    "        self.action_vector = self.get_action_vector()\n",
    "        self.eval_model = DQNModel(input_len, output_len, learning_rate = hyper_params['learning_rate'])\n",
    "        self.target_model = DQNModel(input_len, output_len)\n",
    "        \n",
    "#         memory: Store and sample experience replay.\n",
    "        self.memory = ReplayBuffer_decom(hyper_params['memory_size'])\n",
    "        \n",
    "        \"\"\"\n",
    "            batch_size: Mini batch size for training model.\n",
    "            update_steps: The frequence of traning model\n",
    "            model_replace_freq: The frequence of replacing 'target_model' by 'eval_model'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.update_steps = hyper_params['update_steps']\n",
    "        self.model_replace_freq = hyper_params['model_replace_freq']\n",
    "        \n",
    "#         if os.path.isdir(\"CartPole_summary/Lunarlander/DQN(unconstraint)/\"):\n",
    "#             shutil.rmtree(\"CartPole_summary/Lunarlander/DQN(unconstraint)/\")\n",
    "        \n",
    "        \n",
    "    # Linear decrease function for epsilon\n",
    "    def linear_decrease(self, initial_value, final_value, curr_steps, final_decay_steps):\n",
    "        decay_rate = curr_steps / final_decay_steps\n",
    "        if decay_rate > 1:\n",
    "            decay_rate = 1\n",
    "        return initial_value - (initial_value - final_value) * decay_rate\n",
    "    \n",
    "    def get_action_vector(self):\n",
    "        action_vector = np.zeros((self.action_space, self.action_space))\n",
    "        for i in range(len(action_vector)):\n",
    "            action_vector[i, i] = 1\n",
    "        \n",
    "        return FloatTensor(action_vector)\n",
    "    \n",
    "    def concat_state_action(self, states, actions = None, is_full_action = False):\n",
    "        if is_full_action:\n",
    "            com_state = FloatTensor(states).repeat((1, self.action_space)).view((-1, self.state_len))\n",
    "            actions = self.action_vector.repeat((len(states), 1))\n",
    "        else:\n",
    "            com_state = states.clone()\n",
    "            actions = actions.clone()\n",
    "        state_action = torch.cat((com_state, actions), 1)\n",
    "        return state_action\n",
    "        \n",
    "    def explore_or_exploit_policy(self, state):\n",
    "        p = uniform(0, 1)\n",
    "        # Get decreased epsilon\n",
    "        epsilon = self.linear_decrease(self.initial_epsilon, \n",
    "                               self.final_epsilon,\n",
    "                               self.steps,\n",
    "                               self.epsilon_decay_steps)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        if p < epsilon:\n",
    "            return randint(0, self.action_space - 1)\n",
    "        else:\n",
    "            return self.greedy_policy(state)[0]\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        state_ft = FloatTensor(state).view(-1, self.state_len)\n",
    "        state_action = self.concat_state_action(state_ft, is_full_action = True)\n",
    "        feature_vectors, q_values = self.eval_model.predict_batch(state_action)\n",
    "        q_v, best_action = q_values.max(0)\n",
    "        return best_action.item(), q_v, feature_vectors[best_action.item()]\n",
    "    \n",
    "    def update_batch(self):\n",
    "#         print(self.update_steps)\n",
    "        if len(self.memory) < self.batch_size or self.steps % self.update_steps != 0:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        (states_actions, _, reward, next_states,\n",
    "         is_terminal, _) = batch\n",
    "        \n",
    "#         states_actions = states_actions\n",
    "        next_states = FloatTensor(next_states)\n",
    "        terminal = FloatTensor([1 if t else 0 for t in is_terminal])\n",
    "        reward = FloatTensor(reward)\n",
    "        \n",
    "        batch_index = torch.arange(self.batch_size,\n",
    "                                   dtype=torch.long)\n",
    "        \n",
    "        # Current Q Values\n",
    "        _, q_values = self.eval_model.predict_batch(states_actions)\n",
    "        next_state_actions = self.concat_state_action(next_states, is_full_action = True)\n",
    "        _, q_next = self.target_model.predict_batch(next_state_actions)\n",
    "        q_next = q_next.view((-1, self.action_space))\n",
    "        q_max, idx = q_next.detach().max(1)\n",
    "\n",
    "        q_max = (1 - terminal) * q_max\n",
    "        q_target = reward + self.beta * q_max\n",
    "        q_target = q_target.unsqueeze(1)\n",
    "        \n",
    "        self.eval_model.fit(q_values, q_target)\n",
    "        \n",
    "    def learn_and_evaluate(self, training_episodes, test_interval):\n",
    "        test_number = training_episodes // test_interval\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(test_number):\n",
    "            # learn\n",
    "            self.learn(test_interval)\n",
    "            # evaluate\n",
    "            avg_reward = self.evaluate((i + 1) * test_interval)\n",
    "            all_results.append(avg_reward)\n",
    "            \n",
    "        return all_results\n",
    "    \n",
    "    def get_features_decom(self, state, next_state, done):\n",
    "        \n",
    "        threshold_x = 1\n",
    "        threshold_c_v = 1\n",
    "        threshold_angle = 0.07\n",
    "        threshold_p_v = 0.7\n",
    "        \n",
    "        features_decom = np.ones(self.decom_reward_len)\n",
    "#         cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
    "        next_cart_position, next_cart_velocity, next_pole_angle, next_pole_velocity = next_state\n",
    "            \n",
    "        if threshold_x < next_cart_position:\n",
    "            features_decom[0] = -1\n",
    "        if -threshold_x > next_cart_position:\n",
    "            features_decom[1] = -1        \n",
    "    \n",
    "        if threshold_c_v < next_cart_velocity:\n",
    "            features_decom[2] = -1\n",
    "        if -threshold_c_v > next_cart_velocity:\n",
    "            features_decom[3] = -1   \n",
    "            \n",
    "        if threshold_angle < next_pole_angle:\n",
    "            features_decom[4] = -1\n",
    "        if -threshold_angle > next_pole_angle:\n",
    "            features_decom[5] = -1   \n",
    "        \n",
    "        if threshold_p_v < next_pole_velocity:\n",
    "            features_decom[6] = -1\n",
    "        if -threshold_p_v > next_pole_velocity:\n",
    "            features_decom[7] = -1   \n",
    "        return features_decom\n",
    "    \n",
    "    def learn(self, test_interval):\n",
    "        \n",
    "        for episode in tqdm(range(test_interval), desc=\"Training\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                steps += 1\n",
    "                self.steps += 1\n",
    "                \n",
    "                action = self.explore_or_exploit_policy(state)\n",
    "                next_state, reward, done, _, features_decom = self.env.step(action)\n",
    "                \n",
    "#                 features_decom = self.get_features_decom(state, next_state, steps < self.max_episode_steps and done)\n",
    "                action_vector = np.zeros(self.action_space)\n",
    "                action_vector[action] = 1\n",
    "                \n",
    "                self.memory.add(np.concatenate((state.copy(), action_vector.copy()), axis=0), -1, reward, next_state, steps < self.max_episode_steps and done, features_decom)\n",
    "                self.update_batch()\n",
    "                \n",
    "                if self.steps % self.model_replace_freq == 0:\n",
    "                    if self.model_replace_freq == 1:\n",
    "                        self.target_model.replace_soft(self.eval_model, tau = self.soft_tau)\n",
    "                    else:\n",
    "                        self.target_model.replace(self.eval_model)\n",
    "                state = next_state\n",
    "\n",
    "    def evaluate(self, episode_num, trials = 10):\n",
    "        total_reward = 0\n",
    "        for _ in tqdm(range(trials), desc=\"Evaluating\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < self.max_episode_steps and not done:\n",
    "                steps += 1\n",
    "                action = self.greedy_policy(state)[0]\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "            \n",
    "        avg_reward = total_reward / trials\n",
    "        print(avg_reward)\n",
    "        if avg_reward >= self.best_reward:\n",
    "            self.best_reward = avg_reward\n",
    "            self.save_model()\n",
    "            print(\"save\")\n",
    "#         Writer.add_scalars(main_tag='CartPole/DQN',\n",
    "#                                 tag_scalar_dict = {'DQN(unconstraint)':avg_reward}, \n",
    "# #                                 scalar_value=,\n",
    "#                                 global_step=episode_num)\n",
    "    \n",
    "        return avg_reward\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.eval_model.save(result_floder + '/best_model.pt')\n",
    "        self.memory.save(result_floder)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.eval_model.load(result_floder + '/best_model.pt')\n",
    "        self.memory.load(result_floder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Cart Pole DQN agent\n",
    "Generating policy and feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_episodes, test_interval = 10000, 100\n",
    "agent = DQN_agent(env, hyperparams_Lunarlander)\n",
    "result = agent.learn_and_evaluate(training_episodes, test_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GVF learner\n",
    "Train GVF model base on the dataset and policy above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_GVF_PARAMETERS = {\n",
    "    \"batch size\" : 64, # update batch size\n",
    "    \"learning rate\" : 0.0001,\n",
    "    \"feature num\" : 8, # numbers/length of feature\n",
    "    \"state length\" : 8,\n",
    "    \"discount factor\" : [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99], # for each features respectively\n",
    "    \"action space\": 4,\n",
    "    'model_replace_freq' : 1,\n",
    "    'soft_tau': 0.5\n",
    "}\n",
    "dqn = DQN_agent(env, hyperparams_Lunarlander)\n",
    "dqn.load_model()\n",
    "def policy(state_actions):\n",
    "    _, q_next = dqn.eval_model.predict_batch(state_actions)\n",
    "    q_next = q_next.view((-1, dqn.action_space))\n",
    "    q_max, idx = q_next.detach().max(1)\n",
    "    return idx\n",
    "\n",
    "def get_dataset():\n",
    "    dataset = dqn.memory._storage\n",
    "    print(dataset[:10])\n",
    "    return np.array(dataset).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset()\n",
    "gvf_learner = GVF_learner(CP_GVF_PARAMETERS, dataset, policy)\n",
    "gvf_learner.learn_and_eval(1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# test_data = torch.tensor([[0.015588092617690563, -0.0004975938936695457, 4.084892424316422e-08, 6.387576578781307e-10, 0.0024009563494473696, -9.596109151743804e-08, 1, 1, 0, 0, 1, 0]]).cuda()\n",
    "# print(test_data.size())\n",
    "# r = gvf_learner.eval_model(test_data)\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
